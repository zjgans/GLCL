from __future__ import print_function

import numpy as np
import scipy
from scipy.stats import t
from tqdm import tqdm
from sklearn import metrics
from sklearn.linear_model import LogisticRegression
import torch
import sys, os

sys.path.append(os.path.abspath('..'))

def mean_confidence_interval(data, confidence=0.95):
    a = 100.0 * np.array(data)
    n = len(a)
    m, se = np.mean(a), scipy.stats.sem(a)
    h = se * t._ppf((1+confidence)/2., n-1)
    return m, h


def normalize(x):
    norm = x.pow(2).sum(1, keepdim=True).pow(1. / 2)
    out = x.div(norm)
    return out


def meta_test(net, testloader, is_feat=False, is_norm=True, classifier='LR'):
    net = net.eval()
    acc = []

    with torch.no_grad():
        with tqdm(testloader, total=len(testloader)) as pbar:
            for idx, data in enumerate(pbar):
                support_xs, support_ys, query_xs, query_ys = data

                support_xs = support_xs.cuda()
                query_xs = query_xs.cuda()
                batch_size, _, height, width, channel = support_xs.size()
                support_xs = support_xs.view(-1, height, width, channel)
                query_xs = query_xs.view(-1, height, width, channel)

                if is_feat:
                    feat_support,_ = net(support_xs, is_feat=True)
                    support_features = feat_support[-1].view(support_xs.size(0), -1)
                    feat_query,_= net(query_xs, is_feat=True)
                    query_features = feat_query[-1].view(query_xs.size(0), -1)

                else:
                    support_features = net(support_xs).view(support_xs.size(0), -1)
                    query_features = net(query_xs).view(query_xs.size(0), -1)

                if is_norm:
                    support_features = normalize(support_features)
                    query_features = normalize(query_features)

                support_features = support_features.detach().cpu().numpy()
                query_features = query_features.detach().cpu().numpy()
                
                support_ys = support_ys.view(-1).numpy()
                query_ys = query_ys.view(-1).numpy()
                

                if classifier == 'LR':
                    clf = LogisticRegression(random_state=0, solver='lbfgs', max_iter=1000, penalty='l2',
                                             multi_class='multinomial')
                    clf.fit(support_features, support_ys)
                    query_ys_pred = clf.predict(query_features)
                elif classifier == 'NN':
                    query_ys_pred = NN(support_features, support_ys, query_features)
                elif classifier == 'Cosine':
                    query_ys_pred = Cosine(support_features, support_ys, query_features)
                else:
                    raise NotImplementedError('classifier not supported: {}'.format(classifier))

                acc.append(metrics.accuracy_score(query_ys, query_ys_pred))
                
                pbar.set_postfix({"FSL_Acc":'{0:.2f}'.format(metrics.accuracy_score(query_ys, query_ys_pred))})
    
    return mean_confidence_interval(acc)


def NN(support, support_ys, query):
    """nearest classifier"""
    support = np.expand_dims(support.transpose(), 0)
    query = np.expand_dims(query, 2)

    diff = np.multiply(query - support, query - support)
    distance = diff.sum(1)
    min_idx = np.argmin(distance, axis=1)
    pred = [support_ys[idx] for idx in min_idx]
    return pred


def Cosine(support, support_ys, query):
    """Cosine classifier"""
    support_norm = np.linalg.norm(support, axis=1, keepdims=True)
    support = support / support_norm
    query_norm = np.linalg.norm(query, axis=1, keepdims=True)
    query = query / query_norm

    cosine_distance = query @ support.transpose()
    max_idx = np.argmax(cosine_distance, axis=1)
    pred = [support_ys[idx] for idx in max_idx]
    return pred
